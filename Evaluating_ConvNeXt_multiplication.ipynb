{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8457eec5",
   "metadata": {},
   "source": [
    "## By Dilleswari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b7873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from src.models import Join_fusion\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sys\n",
    "sys.path.append(\"./src\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3f178",
   "metadata": {},
   "source": [
    "Loading Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Join_fusion(\n",
       "  (detection): CDur_fusion(\n",
       "    (gru): GRU(768, 768, batch_first=True, bidirectional=True)\n",
       "    (fusion): Fusion(\n",
       "      (fuse_layer1): conv1d(\n",
       "        (conv): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (fuse_layer2): conv1d(\n",
       "        (conv): Conv1d(768, 3072, kernel_size=(1,), stride=(1,))\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (avg_pool): AvgPool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
       "    )\n",
       "    (fc): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    (outputlayer): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  )\n",
       "  (AudioEncoder): ConvNeXt(\n",
       "    (spectrogram_extractor): Spectrogram(\n",
       "      (stft): STFT(\n",
       "        (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "        (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (logmel_extractor): LogmelFilterBank()\n",
       "    (spec_augmenter): SpecAugmentation(\n",
       "      (time_dropper): DropStripes()\n",
       "      (freq_dropper): DropStripes()\n",
       "    )\n",
       "    (bn0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4), padding=(4, 0))\n",
       "        (1): LayerNorm()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm()\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): LayerNorm()\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): LayerNorm()\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (stages): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Block(\n",
       "          (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Block(\n",
       "          (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Block(\n",
       "          (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm()\n",
       "          (pwconv1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head_audioset): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = {\n",
    "                \"inputdim\" : 64,\n",
    "                \"outputdim\" : 2\n",
    "                }\n",
    "\n",
    "model = Join_fusion(model_config, model_config['inputdim'], model_config[\"outputdim\"])\n",
    "\n",
    "ckp_path = \"./experiments/Join_fusion/2025-07-09_19-54-07_6ab942ac5cd011f08f033cecefb1a652/run_model_9_loss=-0.2484.pt\"\n",
    "ckp = torch.load(ckp_path, map_location='cpu')\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(ckp)\n",
    "    print(\"Successfully loaded model weights\")\n",
    "except:\n",
    "    print(\"Unsuccessful in loading model weights\")\n",
    "\n",
    "model.eval()\n",
    "#summary(model, input_size= [(1,501,64), (1,1001,104)], col_names = [\"input_size\",\"output_size\",\"num_params\"], device='cpu')\n",
    "                            # mixture   reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836ae483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>event_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>4.516348</td>\n",
       "      <td>7.253393</td>\n",
       "      <td>jackhammer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>5.106336</td>\n",
       "      <td>7.081667</td>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>5.813735</td>\n",
       "      <td>7.963874</td>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>6.353976</td>\n",
       "      <td>7.430690</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>8.037359</td>\n",
       "      <td>9.902325</td>\n",
       "      <td>drilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>7.794474</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>gun_shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9952</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>3.391536</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9953</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>1.222317</td>\n",
       "      <td>2.517615</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>5.517929</td>\n",
       "      <td>6.631534</td>\n",
       "      <td>gun_shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9955</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>6.285033</td>\n",
       "      <td>8.152078</td>\n",
       "      <td>street_music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9956 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename     onset     offset  \\\n",
       "0     /home/ananth/datasets/URBAN-SED/audio/test/sou...  4.516348   7.253393   \n",
       "1     /home/ananth/datasets/URBAN-SED/audio/test/sou...  5.106336   7.081667   \n",
       "2     /home/ananth/datasets/URBAN-SED/audio/test/sou...  5.813735   7.963874   \n",
       "3     /home/ananth/datasets/URBAN-SED/audio/test/sou...  6.353976   7.430690   \n",
       "4     /home/ananth/datasets/URBAN-SED/audio/test/sou...  8.037359   9.902325   \n",
       "...                                                 ...       ...        ...   \n",
       "9951  /home/ananth/datasets/URBAN-SED/audio/test/sou...  7.794474  10.000000   \n",
       "9952  /home/ananth/datasets/URBAN-SED/audio/test/sou...  0.016581   3.391536   \n",
       "9953  /home/ananth/datasets/URBAN-SED/audio/test/sou...  1.222317   2.517615   \n",
       "9954  /home/ananth/datasets/URBAN-SED/audio/test/sou...  5.517929   6.631534   \n",
       "9955  /home/ananth/datasets/URBAN-SED/audio/test/sou...  6.285033   8.152078   \n",
       "\n",
       "        event_label  \n",
       "0        jackhammer  \n",
       "1     engine_idling  \n",
       "2     engine_idling  \n",
       "3          dog_bark  \n",
       "4          drilling  \n",
       "...             ...  \n",
       "9951       gun_shot  \n",
       "9952       dog_bark  \n",
       "9953       car_horn  \n",
       "9954       gun_shot  \n",
       "9955   street_music  \n",
       "\n",
       "[9956 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tsv_file = \"./data/flists/urban_sed_test_strong_modified.tsv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6024b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>event_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>4.516348</td>\n",
       "      <td>7.253393</td>\n",
       "      <td>jackhammer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>5.106336</td>\n",
       "      <td>7.081667</td>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>5.813735</td>\n",
       "      <td>7.963874</td>\n",
       "      <td>engine_idling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>6.353976</td>\n",
       "      <td>7.430690</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>8.037359</td>\n",
       "      <td>9.902325</td>\n",
       "      <td>drilling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>7.794474</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>gun_shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9952</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>3.391536</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9953</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>1.222317</td>\n",
       "      <td>2.517615</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>5.517929</td>\n",
       "      <td>6.631534</td>\n",
       "      <td>gun_shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9955</th>\n",
       "      <td>/home/ananth/datasets/URBAN-SED/audio/test/sou...</td>\n",
       "      <td>6.285033</td>\n",
       "      <td>8.152078</td>\n",
       "      <td>street_music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9956 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename     onset     offset  \\\n",
       "0     /home/ananth/datasets/URBAN-SED/audio/test/sou...  4.516348   7.253393   \n",
       "1     /home/ananth/datasets/URBAN-SED/audio/test/sou...  5.106336   7.081667   \n",
       "2     /home/ananth/datasets/URBAN-SED/audio/test/sou...  5.813735   7.963874   \n",
       "3     /home/ananth/datasets/URBAN-SED/audio/test/sou...  6.353976   7.430690   \n",
       "4     /home/ananth/datasets/URBAN-SED/audio/test/sou...  8.037359   9.902325   \n",
       "...                                                 ...       ...        ...   \n",
       "9951  /home/ananth/datasets/URBAN-SED/audio/test/sou...  7.794474  10.000000   \n",
       "9952  /home/ananth/datasets/URBAN-SED/audio/test/sou...  0.016581   3.391536   \n",
       "9953  /home/ananth/datasets/URBAN-SED/audio/test/sou...  1.222317   2.517615   \n",
       "9954  /home/ananth/datasets/URBAN-SED/audio/test/sou...  5.517929   6.631534   \n",
       "9955  /home/ananth/datasets/URBAN-SED/audio/test/sou...  6.285033   8.152078   \n",
       "\n",
       "        event_label  \n",
       "0        jackhammer  \n",
       "1     engine_idling  \n",
       "2     engine_idling  \n",
       "3          dog_bark  \n",
       "4          drilling  \n",
       "...             ...  \n",
       "9951       gun_shot  \n",
       "9952       dog_bark  \n",
       "9953       car_horn  \n",
       "9954       gun_shot  \n",
       "9955   street_music  \n",
       "\n",
       "[9956 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tsv_file = \"./data/flists/urban_sed_test_strong_modified.tsv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab08bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    filename, onset, offset, event_label= row['filename'], row['onset'], row['offset'], row['event_label']\n",
    "    filename = filename.replace(\"./datasets/URBAN-SED/audio/test/\",\"\")\n",
    "\n",
    "    reference_entry = {\n",
    "                            'event_label': event_label,\n",
    "                            'event_onset': onset,\n",
    "                            'event_offset': offset,\n",
    "                            'file': filename\n",
    "                        }\n",
    "    reference_list.append(reference_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1a88a",
   "metadata": {},
   "source": [
    "## Create predicted output\n",
    "### Here we are using the predictions from default threshold which is set to 0.37. Later on, in this notebook we will find the optimum threshold and the performance\n",
    "### Change the file_path in the below cell to load the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e405ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_list = []\n",
    "\n",
    "#file_path = \"./experiments/Join_fusion/2024-08-10_17-15-08_03c282e8570e11efaca83cecefb1a652/hard_predictions_urban_sed_test_strong_threshold_0.37.txt\"\n",
    "#file_path = \"./experiments/Join_fusion/2025-06-30_14-36-06_876fba98559111f0a5da3cecefb1a652/hard_predictions_urban_sed_test_strong_threshold_0.37.txt\"\n",
    "file_path = \"./experiments/Join_fusion/2025-07-09_19-54-07_6ab942ac5cd011f08f033cecefb1a652/hard_predictions_urban_sed_test_strong_threshold_0.37.txt\"\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "for line in lines:\n",
    "    out = line.strip().split('\\t')\n",
    "\n",
    "    filename = out[0]\n",
    "    onset = float(out[1])\n",
    "    offset = float(out[2])\n",
    "    event_label = out[3]\n",
    "\n",
    "    estimated_entry = {\n",
    "                            'event_label': event_label,\n",
    "                            'event_onset': onset,\n",
    "                            'event_offset': offset,\n",
    "                            'file': filename\n",
    "                         }\n",
    "    estimated_list.append(estimated_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3dd079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 38.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment based metrics\n",
      "========================================\n",
      "  Evaluated length                  : 18147.42 sec\n",
      "  Evaluated files                   : 2000 \n",
      "  Segment length                    : 1.00 sec\n",
      "\n",
      "  Overall metrics (micro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 83.62 %\n",
      "    Precision                       : 80.77 %\n",
      "    Recall                          : 86.68 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 0.30 \n",
      "    Substitution rate               : 0.04 \n",
      "    Deletion rate                   : 0.09 \n",
      "    Insertion rate                  : 0.17 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 86.68 %\n",
      "    Specificity                     : 96.59 %\n",
      "    Balanced accuracy               : 91.64 %\n",
      "    Accuracy                        : 95.19 %\n",
      "\n",
      "  Class-wise average metrics (macro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 83.55 %\n",
      "    Precision                       : 81.10 %\n",
      "    Recall                          : 86.39 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 0.34 \n",
      "    Deletion rate                   : 0.14 \n",
      "    Insertion rate                  : 0.21 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 86.39 %\n",
      "    Specificity                     : 96.59 %\n",
      "    Balanced accuracy               : 91.49 %\n",
      "    Accuracy                        : 95.19 %\n",
      "  \n",
      "\n",
      "  Class-wise metrics\n",
      "  ======================================\n",
      "    Event label  | Nref    Nsys  | F        Pre      Rec    | ER       Del      Ins    | Sens     Spec     Bacc     Acc     \n",
      "    ------------ | -----   ----- | ------   ------   ------ | ------   ------   ------ | ------   ------   ------   ------  \n",
      "    air_condit.. | 2693    3359  | 75.7%    68.2%    85.0%  | 0.55     0.15     0.40   | 85.0%    93.3%    89.2%    92.1%   \n",
      "    car_horn     | 2155    2058  | 81.2%    83.1%    79.4%  | 0.37     0.21     0.16   | 79.4%    97.9%    88.6%    95.8%   \n",
      "    children_p.. | 2880    3188  | 83.6%    79.6%    88.1%  | 0.35     0.12     0.23   | 88.1%    95.9%    92.0%    94.7%   \n",
      "    dog_bark     | 2613    2903  | 79.7%    75.7%    84.2%  | 0.43     0.16     0.27   | 84.2%    95.6%    89.9%    94.0%   \n",
      "    drilling     | 2786    2862  | 85.4%    84.2%    86.5%  | 0.30     0.13     0.16   | 86.5%    97.2%    91.9%    95.6%   \n",
      "    engine_idl.. | 2852    3106  | 84.9%    81.4%    88.6%  | 0.32     0.11     0.20   | 88.6%    96.4%    92.5%    95.2%   \n",
      "    gun_shot     | 2288    2177  | 81.4%    83.5%    79.5%  | 0.36     0.21     0.16   | 79.5%    97.8%    88.6%    95.6%   \n",
      "    jackhammer   | 2685    3013  | 91.5%    86.5%    97.1%  | 0.18     0.03     0.15   | 97.1%    97.5%    97.3%    97.4%   \n",
      "    siren        | 2810    2893  | 86.7%    85.4%    88.0%  | 0.27     0.12     0.15   | 88.0%    97.4%    92.7%    95.9%   \n",
      "    street_music | 2752    2893  | 85.3%    83.3%    87.5%  | 0.30     0.12     0.18   | 87.5%    97.0%    92.3%    95.6%   \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sed_eval\n",
    "import dcase_util\n",
    "from tqdm import tqdm\n",
    "\n",
    "reference_event_list = dcase_util.containers.MetaDataContainer(reference_list)\n",
    "estimated_event_list = dcase_util.containers.MetaDataContainer(estimated_list)\n",
    "\n",
    "# segment based metrics, change segment length by modifying time_resolution(in sec)\n",
    "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
    "    event_label_list=reference_event_list.unique_event_labels,\n",
    "    time_resolution=1.0\n",
    ")\n",
    "\n",
    "for filename in tqdm(reference_event_list.unique_files):\n",
    "    \n",
    "    reference_event_list_for_current_file = reference_event_list.filter(filename=filename)\n",
    "    estimated_event_list_for_current_file = estimated_event_list.filter(filename=filename)\n",
    "\n",
    "    segment_based_metrics.evaluate(\n",
    "        reference_event_list=reference_event_list_for_current_file,\n",
    "        estimated_event_list=estimated_event_list_for_current_file\n",
    "    )\n",
    "\n",
    "# print report\n",
    "print(segment_based_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc33ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment-based metrics saved to: /home/ananth/adarsh_Targeted_Audio_Search/experiments/Join_fusion/2025-07-09_19-54-07_6ab942ac5cd011f08f033cecefb1a652/segment_based_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save segment-based metrics report to a file\n",
    "output_path = \"./experiments/Join_fusion/2025-07-09_19-54-07_6ab942ac5cd011f08f033cecefb1a652/segment_based_results.txt\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(str(segment_based_metrics))\n",
    "\n",
    "print(f\"Segment-based metrics saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12895ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c7961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ded63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec24cc54",
   "metadata": {},
   "source": [
    "## Finding the optimum threshold\n",
    "### Evaluating the model to find the best threshold on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a522ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src/')\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from src import dataset, models\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "#from ignite.utils import convert_tensor\n",
    "from ignite.engine import convert_tensor\n",
    "import torch\n",
    "from src import utils\n",
    "import glob\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"   \n",
    "\n",
    "def forward(model, batch):\n",
    "    inputs, frame_level_targets, time, embedding,embed_label, filenames,events = batch\n",
    "    inputs = convert_tensor(inputs, device=DEVICE, non_blocking=True)\n",
    "    frame_level_targets = convert_tensor(frame_level_targets.float(), device=DEVICE, non_blocking=True)\n",
    "    embedding = convert_tensor(embedding, device=DEVICE, non_blocking=True)\n",
    "    embed_label = convert_tensor(embed_label,device=DEVICE,non_blocking=True)\n",
    "    decision,decision_up,logit = model(inputs,embedding)\n",
    "    return decision,decision_up, frame_level_targets, time,embed_label,logit\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "            experiment_path: str,\n",
    "            time_ratio=10. / 1000,\n",
    "            postprocessing='median',\n",
    "            threshold=None,\n",
    "            window_size=None,\n",
    "            **kwargs):\n",
    "    \n",
    "  \n",
    "    config = torch.load(list(Path(f'{experiment_path}').glob(\"run_config*\"))[0], map_location='cpu')\n",
    "    config_parameters = dict(config, **kwargs)\n",
    "\n",
    "    model_parameters = torch.load(glob.glob(\"{}/run_model*\".format(experiment_path))[0],\n",
    "                                    map_location=lambda storage, loc: storage)   \n",
    "    strong_labels_df = pd.read_csv(config_parameters['label'], sep='\\t')\n",
    "\n",
    "    if not np.issubdtype(strong_labels_df['filename'].dtype, np.number):\n",
    "        strong_labels_df['filename'] = strong_labels_df['filename'].apply(os.path.basename)\n",
    "    if 'audiofilepath' in strong_labels_df.columns:  \n",
    "        strong_labels_df['audiofilepath'] = strong_labels_df['audiofilepath'].apply(os.path.basename)\n",
    "        colname = 'audiofilepath'  # AVE\n",
    "    else:\n",
    "        colname = 'filename'  # Dcase etc.\n",
    "    \n",
    "    if \"event_labels\" in strong_labels_df.columns:\n",
    "        assert False, \"Data with the column event_labels are used to train not to evaluate\"\n",
    "    \n",
    "    # Create Test Dataloader\n",
    "    dataloader = dataset.getdataloader_join(\n",
    "        config_parameters['test_data'],\n",
    "        config_parameters['spk_emb_file_path'],\n",
    "        batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Load the model\n",
    "    model = getattr(models, config_parameters['model'])(\n",
    "        model_config=config_parameters,inputdim=64, outputdim=2, **config_parameters['model_args'])\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(model_parameters)\n",
    "        print(\"Successfully loaded model\")\n",
    "    except:\n",
    "        print(\"Unsuccessful in loading model\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    thresholds = np.arange(0.31, 0.51, 0.01)\n",
    "    if window_size is None:\n",
    "        window_size = 1\n",
    "    print(\"Postprocessing: \", postprocessing)\n",
    "    print(\"Window Size: \", window_size)\n",
    "\n",
    "    # In One pass generate all the predictions and save it\n",
    "    predictions_list = [] # to save model predictions\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, unit='file', leave=False): \n",
    "            inputs, frame_level_target, time, embedding,embed_label, filenames,events = batch\n",
    "            decision,decision_up, frame_level_target, time,embed_label,logit = forward(model, batch) \n",
    "            \n",
    "            pred = decision_up.detach().cpu().numpy()\n",
    "            pred = pred[:,:,0]\n",
    "            \n",
    "            predictions_list.append(pred)\n",
    "        print(\"Evaluation dataset :\", len(predictions_list), \"files\")\n",
    "    \n",
    "    # Saving all the predictions for each threshold inside threshold_wise_output folder\n",
    "    folder_name = \"threshold_wise_outputs\"\n",
    "    folder_path = os.path.join(experiment_path, folder_name)\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Now get performance over all the thresholds and save the f-score, accuracy for each threshold\n",
    "    for thres in thresholds:\n",
    "        \n",
    "        print(\"Threshold: \", round(thres,2))\n",
    "        time_predictions = [] # for every threshold we need time predictions of all files which will be stored in this list\n",
    "\n",
    "        for idx, batch in tqdm(enumerate(dataloader), unit='file', leave=False): \n",
    "            inputs, frame_level_target, time, embedding,embed_label, filenames,events = batch\n",
    "            pred = predictions_list[idx]\n",
    "            filtered_pred = utils.median_filter(pred, window_size=window_size, threshold=thres)\n",
    "            \n",
    "            decoded_pred = []\n",
    "            decoded_pred_ = utils.decode_with_timestamps(events[0],filtered_pred[0,:])\n",
    "            if len(decoded_pred_) == 0: # neg deal\n",
    "                decoded_pred_.append((events[0],0,0))\n",
    "            decoded_pred.append(decoded_pred_)\n",
    "\n",
    "            for num_batch in range(len(decoded_pred)): # when we test our model,the batch_size is 1\n",
    "            \n",
    "                filename = filenames[num_batch]\n",
    "                label_prediction = decoded_pred[num_batch]\n",
    "                for event_label, onset, offset in label_prediction:\n",
    "                    time_predictions.append({\n",
    "                        'filename': filename,\n",
    "                        'onset': onset,\n",
    "                        'offset': offset,\n",
    "                        'event_label': event_label})\n",
    "        \n",
    "        assert len(time_predictions) > 0, \"No outputs, lower threshold?\"\n",
    "\n",
    "        pred_df = pd.DataFrame(time_predictions, columns=['filename', 'onset', 'offset','event_label']) \n",
    "        pred_df = utils.predictions_to_time(pred_df, ratio=time_ratio) \n",
    "            \n",
    "        pred_csv_path = os.path.join(folder_path, \"predictions_threshold_{}.txt\".format(round(thres,2)))\n",
    "        pred_df.to_csv(pred_csv_path, index=False, sep=\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30e5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get spk_id_dict and spk_emb_dict\n",
      "Successfully loaded model\n",
      "Postprocessing:  median\n",
      "Window Size:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset : 7702 files\n",
      "Threshold:  0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m postprocessing \u001b[38;5;241m=\u001b[39m config_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[1;32m      9\u001b[0m window_size \u001b[38;5;241m=\u001b[39m config_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m evaluate(experiment_path,\n\u001b[1;32m     12\u001b[0m             time_ratio\u001b[38;5;241m=\u001b[39mtime_ratio,\n\u001b[1;32m     13\u001b[0m             postprocessing\u001b[38;5;241m=\u001b[39mpostprocessing,\n\u001b[1;32m     14\u001b[0m             window_size\u001b[38;5;241m=\u001b[39mwindow_size)\n",
      "Cell \u001b[0;32mIn[11], line 108\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(experiment_path, time_ratio, postprocessing, threshold, window_size, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreshold: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(thres,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    106\u001b[0m time_predictions \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# for every threshold we need time predictions of all files which will be stored in this list\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \n\u001b[1;32m    109\u001b[0m     inputs, frame_level_target, time, embedding,embed_label, filenames,events \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    110\u001b[0m     pred \u001b[38;5;241m=\u001b[39m predictions_list[idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Targeted_Audio_Search_Phase_2/src/dataset.py:264\u001b[0m, in \u001b[0;36mHDF5Dataset_join.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    262\u001b[0m k8_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspk_emb_dict[embed_file_list[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# embedding = self.spk_emb_dict[embed_file_list[0]]\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_fea(k8_path) \u001b[38;5;66;03m# getting features for urban sound 8k file, using concatenated melspecgram and mfcc\u001b[39;00m\n\u001b[1;32m    265\u001b[0m embed_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# label for the weak audio file, one-hot vector\u001b[39;00m\n\u001b[1;32m    266\u001b[0m embed_label_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(embed_file_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Targeted_Audio_Search_Phase_2/src/dataset.py:225\u001b[0m, in \u001b[0;36mHDF5Dataset_join.get_fea\u001b[0;34m(self, path_)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# resample to 16khz\u001b[39;00m\n\u001b[1;32m    224\u001b[0m resample_transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39msr, new_freq\u001b[38;5;241m=\u001b[39mtarget_sr)\n\u001b[0;32m--> 225\u001b[0m audio_mono \u001b[38;5;241m=\u001b[39m resample_transform(audio_mono)\n\u001b[1;32m    227\u001b[0m tempData \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m320000\u001b[39m])\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio_mono\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m320000\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:1010\u001b[0m, in \u001b[0;36mResample.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_freq:\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n\u001b[0;32m-> 1010\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_sinc_resample_kernel(waveform, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth)\n",
      "File \u001b[0;32m~/anaconda3/envs/audition/lib/python3.11/site-packages/torchaudio/functional/functional.py:1550\u001b[0m, in \u001b[0;36m_apply_sinc_resample_kernel\u001b[0;34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[0m\n\u001b[1;32m   1547\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1549\u001b[0m num_wavs, length \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m-> 1550\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(waveform, (width, width \u001b[38;5;241m+\u001b[39m orig_freq))\n\u001b[1;32m   1551\u001b[0m resampled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv1d(waveform[:, \u001b[38;5;28;01mNone\u001b[39;00m], kernel, stride\u001b[38;5;241m=\u001b[39morig_freq)\n\u001b[1;32m   1552\u001b[0m resampled \u001b[38;5;241m=\u001b[39m resampled\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(num_wavs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the above evaluate function\n",
    "experiment_path = \"./experiments/Join_fusion/2025-06-30_14-36-06_876fba98559111f0a5da3cecefb1a652\" \n",
    "time_ratio = 10.0/1000\n",
    "\n",
    "config_file = \"./runconfigs/target_sed_join_train.yaml\"\n",
    "config_parameters = utils.parse_config_or_kwargs(config_file)\n",
    "\n",
    "postprocessing = config_parameters.get('postprocessing', 'double')  \n",
    "window_size = config_parameters.get('window_size', None)\n",
    "\n",
    "evaluate(experiment_path,\n",
    "            time_ratio=time_ratio,\n",
    "            postprocessing=postprocessing,\n",
    "            window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d55348",
   "metadata": {},
   "source": [
    "### The predictions gets saved in the folder named as threshold_wise_outputs for each threshold, and now compare the performance of each threshold with the ground truth by recording F-score and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb53c7",
   "metadata": {},
   "source": [
    "#### Load all the thresholded predicition files and evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a72c1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predictions_threshold_0.1.txt', 'predictions_threshold_0.11.txt', 'predictions_threshold_0.12.txt', 'predictions_threshold_0.13.txt', 'predictions_threshold_0.14.txt', 'predictions_threshold_0.15.txt', 'predictions_threshold_0.16.txt', 'predictions_threshold_0.17.txt', 'predictions_threshold_0.18.txt', 'predictions_threshold_0.19.txt', 'predictions_threshold_0.2.txt', 'predictions_threshold_0.21.txt', 'predictions_threshold_0.22.txt', 'predictions_threshold_0.23.txt', 'predictions_threshold_0.24.txt', 'predictions_threshold_0.25.txt', 'predictions_threshold_0.26.txt', 'predictions_threshold_0.27.txt', 'predictions_threshold_0.28.txt', 'predictions_threshold_0.29.txt', 'predictions_threshold_0.3.txt', 'predictions_threshold_0.31.txt', 'predictions_threshold_0.32.txt', 'predictions_threshold_0.33.txt', 'predictions_threshold_0.34.txt', 'predictions_threshold_0.35.txt', 'predictions_threshold_0.36.txt', 'predictions_threshold_0.37.txt', 'predictions_threshold_0.38.txt', 'predictions_threshold_0.39.txt', 'predictions_threshold_0.4.txt', 'predictions_threshold_0.41.txt', 'predictions_threshold_0.42.txt', 'predictions_threshold_0.43.txt', 'predictions_threshold_0.44.txt', 'predictions_threshold_0.45.txt', 'predictions_threshold_0.46.txt', 'predictions_threshold_0.47.txt', 'predictions_threshold_0.48.txt', 'predictions_threshold_0.49.txt', 'predictions_threshold_0.5.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "threshold_folder_path = os.path.join(experiment_path, \"threshold_wise_outputs\")\n",
    "files = sorted(os.listdir(threshold_folder_path))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c646c",
   "metadata": {},
   "source": [
    "### Create reference output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e74fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sed_eval\n",
    "import dcase_util\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tsv_file = \"./data/flists/urban_sed_test_strong_modified.tsv\"\n",
    "df = pd.read_csv(tsv_file, sep='\\t')\n",
    "\n",
    "reference_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    filename, onset, offset, event_label= row['filename'], row['onset'], row['offset'], row['event_label']\n",
    "    filename = filename.replace(\"/home/ananth/datasets/URBAN-SED/audio/test/\",\"\")\n",
    "\n",
    "    reference_entry = {\n",
    "                            'event_label': event_label,\n",
    "                            'event_onset': onset,\n",
    "                            'event_offset': offset,\n",
    "                            'file': filename\n",
    "                         }\n",
    "    reference_list.append(reference_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d8c4c",
   "metadata": {},
   "source": [
    "### Compute the metrics and save it in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8ef437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [46:18<00:00, 67.78s/it]\n"
     ]
    }
   ],
   "source": [
    "f_measure_list = []\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "threshold_list = []\n",
    "\n",
    "\n",
    "for file in tqdm(files):\n",
    "    \n",
    "    if file == \".ipynb_checkpoints\": # remove .pynb_checkpoints file\n",
    "        continue\n",
    "\n",
    "    threshold = round(float(file.split(\"_\")[-1].replace(\".txt\",\"\")),2)\n",
    "    \n",
    "    file_path = os.path.join(threshold_folder_path, file)\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = lines[1:]\n",
    "\n",
    "    estimated_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        out = line.strip().split('\\t')\n",
    "\n",
    "        filename = out[0]\n",
    "        onset = float(out[1])\n",
    "        offset = float(out[2])\n",
    "        event_label = out[3]\n",
    "\n",
    "        estimated_entry = {\n",
    "                                'event_label': event_label,\n",
    "                                'event_onset': onset,\n",
    "                                'event_offset': offset,\n",
    "                                'file': filename\n",
    "                            }\n",
    "        estimated_list.append(estimated_entry)\n",
    "\n",
    "    reference_event_list = dcase_util.containers.MetaDataContainer(reference_list)\n",
    "    estimated_event_list = dcase_util.containers.MetaDataContainer(estimated_list)\n",
    "\n",
    "    segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
    "        event_label_list=reference_event_list.unique_event_labels,\n",
    "        time_resolution=1.0\n",
    "    )\n",
    "\n",
    "    for filename in reference_event_list.unique_files:\n",
    "        \n",
    "        reference_event_list_for_current_file = reference_event_list.filter(filename=filename)\n",
    "        estimated_event_list_for_current_file = estimated_event_list.filter(filename=filename)\n",
    "\n",
    "        segment_based_metrics.evaluate(\n",
    "            reference_event_list=reference_event_list_for_current_file,\n",
    "            estimated_event_list=estimated_event_list_for_current_file\n",
    "        )\n",
    "\n",
    "    threshold_list.append(threshold)\n",
    "    f_measure_list.append(segment_based_metrics.overall_f_measure()['f_measure'])\n",
    "    precision_list.append(segment_based_metrics.overall_f_measure()['precision'])\n",
    "    recall_list.append(segment_based_metrics.overall_f_measure()['recall'])\n",
    "    accuracy_list.append(segment_based_metrics.overall_accuracy()['accuracy'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "                    \"Threshold\" : threshold_list, \n",
    "                    \"F-score\" : f_measure_list, \n",
    "                    \"Precision\" : precision_list, \n",
    "                    \"Recall\" : recall_list, \n",
    "                    \"Accuracy\" : accuracy_list\n",
    "                })\n",
    "\n",
    "# saving the results of all thresholds in a csv file\n",
    "df.to_csv(experiment_path + 'thresholded_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422a7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F-score: 0.2123317993225643  at threshold: 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "# experiment_path = \"./experiments/Join_fusion/2024-08-10_17-15-08_03c282e8570e11efaca83cecefb1a652/\" \n",
    "df = pd.read_csv(experiment_path + 'thresholded_results.csv')\n",
    "\n",
    "# Get threshold for max F-score\n",
    "max_fscore_idx = df['F-score'].idxmax()\n",
    "max_fscore_threshold = df['Threshold'][max_fscore_idx]\n",
    "max_fscore = df['F-score'][max_fscore_idx]\n",
    "\n",
    "# Get threshold for max Accuracy\n",
    "# max_acc_idx = df['Accuracy'].idxmax()\n",
    "# max_acc_threshold = df['Threshold'][max_acc_idx]\n",
    "# max_acc = df['Accuracy'][max_acc_idx]\n",
    "\n",
    "print(\"Max F-score:\", max_fscore, \" at threshold:\", max_fscore_threshold)\n",
    "# print(\"Max accuracy:\", max_acc, \" at threshold:\", max_acc_threshold)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df['Threshold'].values, df['F-score'].values, label='F-score')\n",
    "plt.plot(df['Threshold'].values, df['Precision'].values, label='Precision')\n",
    "plt.plot(df['Threshold'].values, df['Recall'].values, label='Recall')\n",
    "# plt.plot(df['Threshold'].values, df['Accuracy'].values, label='Accuracy')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398ebdc",
   "metadata": {},
   "source": [
    "### Evaluating the performance of the model at the threshold that gives the maximum F-score\n",
    "### best_threshold_file will contain the path to the predictions file which gave best results. So, evaluate the classwise results with this threshold now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d09f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresh_file = threshold_folder_path + \"/predictions_threshold_{}.txt\".format(max_fscore_threshold)\n",
    "estimated_list = []\n",
    "\n",
    "with open(best_thresh_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines = lines[1:]\n",
    "\n",
    "for line in lines:\n",
    "    out = line.strip().split('\\t')\n",
    "\n",
    "    filename = out[0]\n",
    "    onset = float(out[1])\n",
    "    offset = float(out[2])\n",
    "    event_label = out[3]\n",
    "\n",
    "    estimated_entry = {\n",
    "                            'event_label': event_label,\n",
    "                            'event_onset': onset,\n",
    "                            'event_offset': offset,\n",
    "                            'file': filename\n",
    "                         }\n",
    "    estimated_list.append(estimated_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c29c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:21<00:00, 24.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment based metrics\n",
      "========================================\n",
      "  Evaluated length                  : 36524.85 sec\n",
      "  Evaluated files                   : 2000 \n",
      "  Segment length                    : 1.00 sec\n",
      "\n",
      "  Overall metrics (micro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 21.23 %\n",
      "    Precision                       : 14.60 %\n",
      "    Recall                          : 38.89 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 2.68 \n",
      "    Substitution rate               : 0.21 \n",
      "    Deletion rate                   : 0.40 \n",
      "    Insertion rate                  : 2.07 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 38.89 %\n",
      "    Specificity                     : 82.96 %\n",
      "    Balanced accuracy               : 60.92 %\n",
      "    Accuracy                        : 79.88 %\n",
      "\n",
      "  Class-wise average metrics (macro-average)\n",
      "  ======================================\n",
      "  F-measure\n",
      "    F-measure (F1)                  : 20.84 %\n",
      "    Precision                       : 14.35 %\n",
      "    Recall                          : 38.37 %\n",
      "  Error rate\n",
      "    Error rate (ER)                 : 2.88 \n",
      "    Deletion rate                   : 0.62 \n",
      "    Insertion rate                  : 2.27 \n",
      "  Accuracy\n",
      "    Sensitivity                     : 38.37 %\n",
      "    Specificity                     : 82.94 %\n",
      "    Balanced accuracy               : 60.66 %\n",
      "    Accuracy                        : 79.88 %\n",
      "  \n",
      "\n",
      "  Class-wise metrics\n",
      "  ======================================\n",
      "    Event label  | Nref    Nsys  | F        Pre      Rec    | ER       Del      Ins    | Sens     Spec     Bacc     Acc     \n",
      "    ------------ | -----   ----- | ------   ------   ------ | ------   ------   ------ | ------   ------   ------   ------  \n",
      "    air_condit.. | 2693    8936  | 24.3%    15.8%    52.5%  | 3.27     0.47     2.79   | 52.5%    78.7%    65.6%    76.9%   \n",
      "    car_horn     | 2155    4651  | 14.5%    10.6%    22.9%  | 2.70     0.77     1.93   | 22.9%    88.4%    55.7%    84.7%   \n",
      "    children_p.. | 2880    7278  | 22.2%    15.5%    39.1%  | 2.75     0.61     2.14   | 39.1%    82.5%    60.8%    79.2%   \n",
      "    dog_bark     | 2613    7652  | 19.6%    13.1%    38.5%  | 3.16     0.62     2.54   | 38.5%    81.2%    59.9%    78.3%   \n",
      "    drilling     | 2786    7228  | 22.3%    15.4%    40.0%  | 2.79     0.60     2.19   | 40.0%    82.7%    61.3%    79.5%   \n",
      "    engine_idl.. | 2852    7705  | 22.5%    15.4%    41.6%  | 2.87     0.58     2.29   | 41.6%    81.5%    61.5%    78.5%   \n",
      "    gun_shot     | 2288    5437  | 16.0%    11.4%    27.0%  | 2.84     0.73     2.11   | 27.0%    86.5%    56.8%    82.9%   \n",
      "    jackhammer   | 2685    7085  | 22.2%    15.3%    40.4%  | 2.83     0.60     2.23   | 40.4%    83.0%    61.7%    80.0%   \n",
      "    siren        | 2810    6880  | 21.6%    15.2%    37.2%  | 2.70     0.63     2.08   | 37.2%    83.4%    60.3%    80.0%   \n",
      "    street_music | 2752    7765  | 23.2%    15.7%    44.4%  | 2.93     0.56     2.38   | 44.4%    81.5%    62.9%    78.8%   \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reference_event_list = dcase_util.containers.MetaDataContainer(reference_list)\n",
    "estimated_event_list = dcase_util.containers.MetaDataContainer(estimated_list)\n",
    "\n",
    "# segment based metrics, change segment length by modifying time_resolution(in sec)\n",
    "segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
    "    event_label_list=reference_event_list.unique_event_labels,\n",
    "    time_resolution=1.0\n",
    ")\n",
    "\n",
    "for filename in tqdm(reference_event_list.unique_files):\n",
    "    \n",
    "    reference_event_list_for_current_file = reference_event_list.filter(filename=filename)\n",
    "    estimated_event_list_for_current_file = estimated_event_list.filter(filename=filename)\n",
    "\n",
    "    segment_based_metrics.evaluate(\n",
    "        reference_event_list=reference_event_list_for_current_file,\n",
    "        estimated_event_list=estimated_event_list_for_current_file\n",
    "    )\n",
    "\n",
    "# print report\n",
    "print(segment_based_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
